{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a517e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import datafree\n",
    "%config InlineBackend.figure_format = 'pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a8453e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 27 04:17:01 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.60.02    Driver Version: 510.60.02    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:05:00.0 Off |                  N/A |\n",
      "| 53%   51C    P2   178W / 350W |  20079MiB / 24576MiB |     37%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:09:00.0 Off |                  N/A |\n",
      "| 62%   62C    P2   317W / 350W |  18797MiB / 24576MiB |     90%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:0A:00.0 Off |                  N/A |\n",
      "| 96%   70C    P2   338W / 350W |  15453MiB / 24576MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:82:00.0 Off |                  N/A |\n",
      "| 71%   68C    P2   340W / 350W |  15453MiB / 24576MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce ...  Off  | 00000000:85:00.0 Off |                  N/A |\n",
      "| 69%   64C    P2   335W / 350W |  22520MiB / 24576MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce ...  Off  | 00000000:89:00.0 Off |                  N/A |\n",
      "| 75%   66C    P2   339W / 350W |  15453MiB / 24576MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     32019      C   python                          20077MiB |\n",
      "|    1   N/A  N/A     26665      C   python                          18795MiB |\n",
      "|    2   N/A  N/A     11824      C   python                          15451MiB |\n",
      "|    3   N/A  N/A     14467      C   python                          15451MiB |\n",
      "|    4   N/A  N/A      3120      C   python                           7001MiB |\n",
      "|    4   N/A  N/A     15552      C   python                          15517MiB |\n",
      "|    5   N/A  N/A     17490      C   python                          15451MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "821287c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Time(h)  Acc@1      method  size\n",
      "0  11.900000  81.10        DAFL   300\n",
      "1  23.458333  89.46         ADI   300\n",
      "2  12.972222  90.84         DFQ   300\n",
      "3  69.583333  91.13         CMI   300\n",
      "4  13.050000  91.61      CuDFKD   300\n",
      "5  14.466667  92.04  AdaDFKD(S)  5000\n",
      "6  15.566667  92.19  AdaDFKD(G)  5000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "accs = {\n",
    "    'DAFL': [11.9, 81.10],\n",
    "    'ADI': [(28 / 60 + 9 / 3600)*50, 89.46],\n",
    "    'DFQ': [(0.25 + 34/3600)*50, 90.84],\n",
    "    'CMI': [(1 + 23.5/60)*50, 91.13],\n",
    "    'CuDFKD': [13 + 3/60, 91.61],\n",
    "    'AdaDFKD(S)': [14 + 28/60, 92.04],\n",
    "    'AdaDFKD(G)': [15 + 34/60, 92.19]\n",
    "}\n",
    "\n",
    "acc = {\n",
    "    'Time(h)': [11.9, (28 / 60 + 9 / 3600)*50,(0.25 + 34/3600)*50, (1 + 23.5/60)*50, 13 + 3/60,14 + 28/60, 15 + 34/60 ],\n",
    "    'Acc@1':[81.10,  89.46, 90.84, 91.13, 91.61, 92.04, 92.19],\n",
    "    'method': accs.keys(),\n",
    "    'size': [300, 300, 300, 300, 300, 5000, 5000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(acc)\n",
    "df.to_csv('acc.csv', index=None)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "343196b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_df(ax, df):\n",
    "    for i in range(len(df)):\n",
    "        method = df.loc[i]['method']\n",
    "        acc1 = df.loc[i]['Acc@1']\n",
    "        time = df.loc[i]['Time(h)']\n",
    "        if method == 'DFQ' or method == 'CuDFKD':\n",
    "            x_shift, y_shift = -3, 0\n",
    "        else:\n",
    "            x_shift, y_shift = -1, 0.8\n",
    "        ax.text(time+x_shift, acc1+y_shift, method, color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6d28ded",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/pdf": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "markers = {\"AdaDFKD(S)\": \"*\", \"AdaDFKD(G)\": \"*\", \"CuDFKD\": \"o\", \"CMI\":\"o\", \"DFQ\":\"o\", \"ADI\":\"o\", \"DAFL\":\"o\"}\n",
    "ax = sns.scatterplot(data=df, x='Time(h)', y='Acc@1', markers=markers, s=[50, 50, 50, 50, 50, 300, 300])\n",
    "text_df(ax, df)\n",
    "# ax.set_xscale('log', base=2, subs=[1.25, 1.5])\n",
    "# ax.set_xlim(7, 64)\n",
    "# ax.set_xticklabels([ 0, 8, 16, 32, 64])\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c394b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7f81d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed = False\n",
    "gpu = 5\n",
    "# gpu ='0,1'\n",
    "batch_size = 128\n",
    "workers = 8\n",
    "num_classes = 10\n",
    "# num_classes = 100\n",
    "# num_classes = 200\n",
    "def prepare_model(model):\n",
    "    if not torch.cuda.is_available():\n",
    "        print('using CPU, this will be slow')\n",
    "        return model\n",
    "    elif distributed:\n",
    "        # For multiprocessing distributed, DistributedDataParallel constructor\n",
    "        # should always set the single device scope, otherwise,\n",
    "        # DistributedDataParallel will use all available devices.\n",
    "        if gpu is not None:\n",
    "#             torch.cuda.set_device(gpu)\n",
    "            model.cuda()\n",
    "            # When using a single GPU per process and per\n",
    "            # DistributedDataParallel, we need to divide the batch size\n",
    "            # ourselves based on the total number of GPUs we have\n",
    "            batch_size = int(batch_size / 1)\n",
    "            workers = int((workers + 1 - 1) / 1)\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[int(x) for x in gpu.split(',')])\n",
    "            return model\n",
    "        else:\n",
    "            model.cuda()\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "            return model\n",
    "    elif gpu is not None:\n",
    "        torch.cuda.set_device(gpu)\n",
    "        model = model.cuda(gpu)\n",
    "        return model\n",
    "    else:\n",
    "        # DataParallel will divide and allocate batch_size to all available GPUs\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddea5407",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10,CIFAR100\n",
    "import datafree\n",
    "import registry\n",
    "from torch import nn\n",
    "student = registry.get_model('resnet18', num_classes=num_classes)\n",
    "teacher = registry.get_model('resnet34', num_classes=num_classes, pretrained=True).eval()\n",
    "# student = registry.get_model('wrn40_1', num_classes=num_classes)\n",
    "# student= registry.get_model('wrn16_2', num_classes=num_classes)\n",
    "# teacher = registry.get_model('wrn40_2', num_classes=num_classes)\n",
    "# normalizer = datafree.utils.Normalizer(**registry.NORMALIZE_DICT['tiny_imagenet'])\n",
    "# normalizer = datafree.utils.Normalizer(**registry.NORMALIZE_DICT['cifar10'])\n",
    "normalizer = datafree.utils.Normalizer(**registry.NORMALIZE_DICT['cifar100'])\n",
    "student = prepare_model(student)\n",
    "\n",
    "# teacher = teacher.to(gpu)\n",
    "# teacher.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "# num_ftrs = teacher.fc.in_features\n",
    "# teacher.fc = nn.Linear(num_ftrs, 200)\n",
    "# teacher.conv1 = nn.Conv2d(3,64, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "# teacher.maxpool = nn.Sequential()\n",
    "teacher = prepare_model(teacher)\n",
    "# ckpt = torch.load('../checkpoints/scratch/tiny_imagenet_resnet34_imagenet.pth', map_location='cpu')\n",
    "# dict_ckpt = dict()\n",
    "# for k, v in ckpt['state_dict'].items():\n",
    "#     dict_ckpt['.'.join(k.split('.')[1:])] = v\n",
    "# teacher.load_state_dict(dict_ckpt)\n",
    "\n",
    "teacher.load_state_dict(torch.load('../checkpoints/scratch/cifar10_resnet34.pth', map_location='cpu')['state_dict'])\n",
    "# teacher.load_state_dict(torch.load('../checkpoints/scratch/cifar10_wrn40_2.pth', map_location='cpu')['state_dict'])\n",
    "# teacher.load_state_dict(torch.load('../checkpoints/scratch/cifar100_wrn40_2.pth', map_location='cpu')['state_dict'])\n",
    "# print(ckpt['best_acc1'])\n",
    "# teacher.load_state_dict(torch.load('../checkpoints/scratch/cifar100_resnet34.pth', map_location='cpu')['state_dict'])\n",
    "teacher.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41af454a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '../run/cr4_sim_normalize_pos_c': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../run/cr4_sim_normalize_pos_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "353c21e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../run/cr6_sim_normalize_pos/buffer.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8fac59dbbf66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manchor_bank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../run/cr6_sim_normalize_pos/buffer.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# all_anchor = anchor_bank.reshape(-1, 512)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor_bank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../run/cr6_sim_normalize_pos/buffer.pt'"
     ]
    }
   ],
   "source": [
    "anchor_bank = torch.load('../run/cr6_sim_normalize_pos/buffer.pt', map_location='cpu')\n",
    "# all_anchor = anchor_bank.reshape(-1, 512)\n",
    "print(anchor_bank.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3b27168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def difficulty_loss(anchor, teacher, t_out, logit_t, ds='cifar10', hard_factor=0., tau=10, device='cpu', d_neg_fea=None):\n",
    "    batch_size = anchor.size(0)\n",
    "    with torch.no_grad():\n",
    "        # t_logit, anchor_t_out = teacher(anchor.to(device).detach(), return_features=True)\n",
    "        t_logit = teacher(anchor.to(device).detach())\n",
    "        anchor_t_out = anchor.to(device)\n",
    "        # pseudo_label = pseudo_label.argmax(1)\n",
    "    # loss = 0.\n",
    "    pos_loss = 0.\n",
    "    neg_loss = 0.\n",
    "    if ds == 'cifar10':\n",
    "        normalized_anchor_t_out, normalized_t_out = F.normalize(anchor_t_out, dim=1), F.normalize(t_out, dim=1)\n",
    "        d = torch.mm(normalized_anchor_t_out, normalized_t_out.T)\n",
    "        N_an, N_batch = d.size()\n",
    "        \n",
    "        sorted_d, indice_d = torch.sort(d, dim=1)\n",
    "        d_pos = sorted_d[:, -int(0.05 * N_batch):]\n",
    "        d_neg = sorted_d[:, :-int(0.05 * N_batch)]\n",
    "        # n_neg = d_neg.size(1)\n",
    "        d_mask = torch.zeros_like(indice_d)\n",
    "        d_mask = d_mask.scatter(1, indice_d[:, -int(0.1*N_batch):], 1)\n",
    "        p_t_anchor = torch.softmax(t_logit, 1)\n",
    "        p_t_batch = torch.softmax(logit_t, 1)\n",
    "        kld_matrix = -torch.mm(p_t_anchor, p_t_batch.T.log()) + torch.diag(torch.mm(p_t_anchor, p_t_anchor.T.log())).unsqueeze(1)\n",
    "        l_kld = ((kld_matrix * d_mask).sum(1) / d_mask.sum(1)).mean()\n",
    "        # Get positive DA index\n",
    "        p_pos = torch.softmax(d_pos / tau, dim=1)\n",
    "        p_da_pos = torch.quantile(p_pos, q=1-hard_factor, dim=1).unsqueeze(1)\n",
    "        pos_loss = torch.sum(p_pos * torch.log(p_pos / p_da_pos).abs(), dim=1).mean()\n",
    "        # Get Negative DA index\n",
    "        \n",
    "        if d_neg_fea is not None:\n",
    "            d = torch.cat([d_neg, d_pos], 1)\n",
    "            d_mask = torch.zeros_like(d)\n",
    "#             d_mask[:, ]\n",
    "        p_total = torch.softmax(d / tau, dim=1)\n",
    "        # Out supervised loss.\n",
    "        print(d_mask, d_mask.shape)\n",
    "        neg_loss = -((d_mask * p_total.log()).sum(1) / (d_mask.sum(1))).mean()\n",
    "#         print(pos_loss, neg_loss, l_kld)\n",
    "        \n",
    "        return pos_loss, indice_d, neg_loss, l_kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3874fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DCGAN_Generator_CIFAR10(\n",
       "  (project): Sequential(\n",
       "    (0): Flatten()\n",
       "    (1): Linear(in_features=512, out_features=16384, bias=True)\n",
       "  )\n",
       "  (main): Sequential(\n",
       "    (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (9): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg = datafree.models.generator.DCGAN_Generator_CIFAR10(nz=512, ngf=64, nc=3, img_size=32, d=2, cond=False, type='normal', widen_factor=1)\n",
    "tg.load_state_dict(torch.load('../checkpoints/datafree-improved_cudfkd/cifar10-resnet34-resnet18--infonce_s_exp6.pth', map_location='cpu')['G'])\n",
    "prepare_model(tg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dec2cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8, 8, 8, 2, 3, 8, 8, 2, 6, 6], device='cuda:5')\n"
     ]
    }
   ],
   "source": [
    "student.load_state_dict(torch.load('../checkpoints/datafree-improved_cudfkd/cifar10-resnet34-resnet18--infonce_s_exp6.pth', map_location='cpu')['state_dict'])\n",
    "z = torch.randn(512, 512).to(gpu)\n",
    "x = normalizer(tg(z))\n",
    "t_out, t_feat = teacher(x, return_features=True)\n",
    "\n",
    "s_out, s_feat= student(x, return_features=True)\n",
    "tau = 0.07\n",
    "t_feat = torch.nn.functional.normalize(t_feat, dim=-1)\n",
    "s_feat = torch.nn.functional.normalize(s_feat, dim=-1)\n",
    "\n",
    "l_neg = t_feat @ s_feat.T\n",
    "_, indice_neg = torch.sort(l_neg, dim=1)\n",
    "\n",
    "balance = torch.nn.functional.cross_entropy(t_out, t_out.argmax(1), reduction='none')\n",
    "_ , index = torch.sort(balance)\n",
    "img_anchor = x[index[:10]].cpu()\n",
    "label = t_out[index[:10]].argmax(1)\n",
    "print(label)\n",
    "new_img = x.cpu()[indice_neg[index[:10]]]\n",
    "imgs = new_img.reshape(5120, 3, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc50f321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.5971e-02, 4.9913e-03, 4.8435e-04, 1.1491e-04, 6.2307e-02, 5.7498e-03,\n",
      "        1.7951e-04, 6.6437e-04, 4.7887e-04, 6.1286e-01, 1.2127e-02, 3.6090e-04,\n",
      "        1.7019e-03, 1.2994e-05, 2.5305e-02, 1.5854e-04, 4.3419e-04, 1.8120e-05,\n",
      "        1.4477e-01, 3.5065e-04, 1.4989e-03, 1.1420e-04, 2.0437e-01, 2.1738e-02,\n",
      "        3.8385e-05, 7.2150e-01, 8.9808e-04, 1.7343e-04, 3.8223e-04, 1.4066e-04,\n",
      "        3.9219e-05, 8.0208e-04, 5.1520e-01, 1.2898e-04, 1.2065e-01, 3.7653e-03,\n",
      "        2.8852e-03, 6.1629e-05, 4.8340e-04, 5.2118e-02, 2.6861e-02, 2.1320e-01,\n",
      "        2.8856e-04, 2.6995e-01, 4.6598e-01, 2.6723e-04, 5.1318e-04, 2.8630e-03,\n",
      "        3.2706e-04, 3.8578e-03, 2.0919e-01, 2.8749e-04, 4.1369e-04, 6.8200e-04,\n",
      "        5.4666e-03, 1.8004e-02, 6.1529e-04, 1.0475e+00, 3.0418e-04, 4.0590e-02,\n",
      "        2.5578e-02, 2.0443e-01, 8.7438e-04, 4.4751e-02, 1.0169e-02, 2.3386e-04,\n",
      "        7.3311e-05, 1.8726e-04, 1.7403e-04, 4.6731e-04, 1.4777e-03, 7.5695e-05,\n",
      "        9.5024e-04, 7.8147e-04, 3.0604e-01, 2.2397e-04, 4.5861e-04, 8.3168e-03,\n",
      "        7.1237e-03, 7.8742e-04, 9.9654e-05, 1.8704e-01, 4.0932e-02, 2.4328e-04,\n",
      "        1.7641e-03, 3.4072e-03, 3.4778e-02, 1.4709e-04, 4.3673e-02, 4.9424e-01,\n",
      "        1.7933e-01, 3.1049e-04, 6.2306e-01, 3.1669e-04, 5.9131e-01, 1.0466e-04,\n",
      "        2.5961e-01, 1.2335e-03, 5.7597e-04, 2.1134e-04, 4.2249e-03, 2.0417e-01,\n",
      "        2.8888e-02, 8.4829e-04, 5.0568e-04, 1.2931e-02, 2.0357e-01, 1.1072e+00,\n",
      "        4.4622e-04, 4.9416e-03, 9.9050e-04, 3.6802e-01, 3.4465e-01, 2.8215e-03,\n",
      "        5.7254e-01, 1.8236e-02, 4.3749e-05, 1.9860e-03, 1.5424e-04, 1.3196e-04,\n",
      "        1.4101e-04, 2.1026e-04, 6.1391e-05, 6.8365e-03, 5.6902e-02, 3.1728e-04,\n",
      "        1.0644e-02, 2.0434e-01, 6.0791e-02, 2.8940e-04, 1.4796e-03, 3.7311e-01,\n",
      "        2.7584e-02, 3.2038e-02, 1.0847e-04, 1.2504e-04, 1.1483e-02, 4.6781e-03,\n",
      "        2.6619e-01, 1.3351e-05, 1.4029e-03, 2.9173e-01, 1.3381e-01, 2.5376e-04,\n",
      "        6.3971e-04, 2.5128e-02, 1.5957e-03, 3.2309e-01, 1.6326e-02, 2.5736e-03,\n",
      "        7.0273e-04, 1.4221e-04, 1.8642e-02, 1.5079e-04, 6.9657e-01, 8.7377e-05,\n",
      "        9.0357e-05, 1.0689e-02, 8.5568e-04, 7.9415e-02, 1.9870e-04, 1.0015e-01,\n",
      "        2.4387e-04, 3.2312e-04, 1.0700e-02, 1.9970e-01, 4.4610e-04, 2.0678e-03,\n",
      "        1.9398e-01, 1.9437e-03, 2.4256e-04, 6.3459e-04, 2.2635e-04, 8.0601e-04,\n",
      "        1.0898e-03, 7.0927e-05, 2.0947e-02, 3.0899e-01, 9.7455e-02, 9.6441e-04,\n",
      "        6.0469e-04, 9.1549e-05, 6.0838e-04, 6.3775e-01, 1.0997e-01, 1.9432e-02,\n",
      "        2.8804e-02, 6.2583e-05, 6.6807e-03, 8.4059e-02, 1.0931e-04, 4.3454e-04,\n",
      "        1.8374e-03, 5.6191e-04, 1.1612e-01, 3.1319e-01, 8.5206e-01, 9.6361e-02,\n",
      "        2.9018e-01, 3.9942e-01, 5.4070e-03, 6.4734e-01, 1.4483e-04, 3.3826e-04,\n",
      "        3.4966e-01, 8.6788e-03, 9.8321e-02, 2.5960e-04, 3.0143e-04, 5.3094e-04,\n",
      "        1.6367e-02, 7.0450e-05, 3.4229e-01, 2.1315e-03, 1.3885e-02, 3.2467e-04,\n",
      "        3.2097e-02, 2.2401e-01, 9.8920e-03, 4.5418e-05, 2.9804e-03, 8.3424e-04,\n",
      "        6.2196e-04, 5.3091e-01, 5.0520e-04, 3.3506e-03, 3.9245e-03, 1.1400e-02,\n",
      "        1.6482e-02, 6.9219e-02, 2.6318e-04, 7.7397e-03, 5.0186e-05, 3.4148e-04,\n",
      "        2.2977e-01, 3.1121e-04, 1.0002e-02, 7.0450e-05, 4.5000e-02, 2.1085e-01,\n",
      "        3.8517e-01, 7.3440e-03, 2.7727e-03, 2.0442e-04, 3.2975e-03, 1.0883e-04,\n",
      "        5.1453e-01, 1.3068e-03, 5.4690e-04, 1.9148e-03, 9.9669e-04, 1.9300e-02,\n",
      "        1.0790e-03, 1.4280e-04, 6.2947e-04, 4.8947e-04, 9.9633e-04, 2.4609e-03,\n",
      "        3.7079e-04, 1.7916e-04, 3.1037e-01, 6.0676e-05, 1.2676e-03, 2.3948e-02,\n",
      "        6.2415e-02, 4.2060e-04, 4.0556e-02, 6.0683e-04, 1.4645e-02, 7.0875e-02,\n",
      "        2.3819e-03, 8.9430e-01, 3.2050e-04, 5.1222e-01, 4.0420e-01, 2.8513e-01,\n",
      "        4.0857e-04, 7.9598e-03, 4.9122e-01, 8.8449e-05, 7.3182e-02, 4.3131e-03,\n",
      "        2.1703e-02, 1.0408e-01, 2.5446e-03, 6.4376e-04, 2.6690e-01, 1.1728e-01,\n",
      "        2.4576e-02, 1.2439e-01, 1.2080e-01, 6.7259e-04, 3.0160e-01, 9.0120e-02,\n",
      "        2.0082e-03, 5.0373e-02, 2.0659e-01, 6.2303e-04, 3.7413e-04, 4.0030e-03,\n",
      "        1.4566e-04, 3.5371e-02, 1.4684e-01, 7.3669e-05, 1.5889e-02, 2.8504e-02,\n",
      "        2.8014e-05, 4.1461e-02, 1.9310e-04, 1.6447e-03, 5.8026e-04, 4.0338e-03,\n",
      "        4.6874e-04, 5.4883e-03, 2.7677e-04, 4.0247e-01, 2.2964e-02, 2.9161e-02,\n",
      "        3.4227e-01, 5.3643e-05, 7.9033e-05, 1.5069e-01, 5.6001e-04, 8.2185e-04,\n",
      "        1.8845e-04, 1.4409e-03, 5.2169e-02, 2.5519e-04, 2.5629e-03, 5.2057e-04,\n",
      "        3.6742e-02, 1.7129e-03, 7.2596e-05, 5.1688e-04, 2.3613e-04, 1.0003e-01,\n",
      "        2.6044e-04, 8.2254e-03, 1.1932e-04, 2.2372e-02, 3.9839e-02, 6.2339e-04,\n",
      "        8.6345e-03, 1.5202e-01, 1.4554e-04, 3.9772e-04, 1.5384e-03, 2.0268e-03,\n",
      "        9.3337e-05, 2.4602e-04, 2.6807e-04, 1.1421e-03, 3.4303e-04, 2.0981e-05,\n",
      "        2.6294e-03, 1.3577e-04, 3.9949e-02, 7.2119e-05, 3.9701e-04, 9.7747e-05,\n",
      "        4.8384e-01, 1.3351e-04, 2.2981e-04, 1.6736e-04, 5.3640e-02, 3.9297e-01,\n",
      "        6.2828e-03, 2.7817e-01, 1.4983e-04, 6.9151e-03, 5.3732e-03, 5.1402e-04,\n",
      "        4.0892e-04, 1.8171e-03, 1.0062e-03, 1.4654e-01, 1.0728e-04, 2.0836e-04,\n",
      "        1.2404e-01, 5.3404e-05, 2.9064e-03, 6.3298e-05, 6.9139e-05, 7.7960e-05,\n",
      "        3.0210e-03, 1.0001e-01, 1.5785e-03, 9.4171e-05, 4.5015e-04, 5.5817e-03,\n",
      "        4.5529e-02, 5.7335e-04, 4.1608e-01, 1.0037e-04, 4.7843e-01, 5.6287e-04,\n",
      "        3.6259e-01, 1.0917e-02, 6.8436e-02, 1.2681e-01, 3.3223e-03, 1.4435e-04,\n",
      "        1.1833e-03, 7.3120e-04, 2.7701e-01, 2.0037e-04, 4.8280e-04, 2.0318e-03,\n",
      "        1.0101e-01, 9.6477e-04, 1.7228e-01, 8.0225e-05, 7.5169e-04, 3.5510e-02,\n",
      "        3.8471e-02, 4.3932e-01, 2.3745e-01, 1.1719e-02, 2.9166e-04, 2.6610e-03,\n",
      "        4.6555e-03, 4.2572e-04, 4.8008e-03, 3.2150e-03, 1.2163e-02, 8.2609e-05,\n",
      "        5.9122e-04, 8.3038e-01, 3.1049e-04, 4.0282e-01, 1.8142e-04, 4.7017e-04,\n",
      "        2.2228e-02, 9.3170e-02, 9.9219e-03, 1.8905e-01, 2.3184e-04, 1.2627e-03,\n",
      "        1.7213e-03, 1.0465e-03, 2.1777e-04, 9.6446e-03, 2.6950e-04, 1.0540e-01,\n",
      "        3.4256e-01, 5.9742e-04, 6.2481e-03, 4.0234e-03, 1.8392e-04, 2.9993e-02,\n",
      "        4.7887e-04, 4.8268e-02, 1.0443e-01, 2.1126e-01, 7.9718e-02, 1.2314e-04,\n",
      "        1.1660e-02, 5.4154e-04, 5.3184e-01, 6.9177e-03, 3.7073e-05, 3.7603e-04,\n",
      "        1.4852e-03, 1.4514e-03, 4.3206e-03, 1.6462e-02, 3.7794e-04, 4.6952e-02,\n",
      "        2.0303e-03, 2.8451e-04, 8.9641e-05, 1.4604e-01, 7.7301e-02, 3.3269e-01,\n",
      "        2.3219e-04, 4.5867e-02, 9.9586e-02, 1.1522e-03, 1.8120e-05, 2.3863e-03,\n",
      "        2.4733e-04, 5.1688e-04, 6.8951e-03, 4.9722e-04, 1.1236e-01, 2.1169e-04,\n",
      "        1.5794e-04, 1.2385e-04, 3.9736e-04, 3.0632e-04, 3.2086e-04, 1.2119e-03,\n",
      "        6.9549e-01, 2.2534e-03, 1.9870e-04, 6.2732e-04, 2.1940e-03, 2.0418e-04,\n",
      "        1.4364e-04, 1.2963e-01, 5.3524e-05, 3.8235e-04, 4.5903e-01, 3.2879e-03,\n",
      "        2.3579e-02, 4.2346e-04, 7.2866e-01, 2.3936e-03, 4.1262e-04, 2.2457e-04,\n",
      "        3.6078e-04, 1.6652e-04], device='cuda:5', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cb60d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0100)\n",
      "tensor(0.1100)\n",
      "tensor(0.2100)\n",
      "tensor(0.3100)\n",
      "tensor(0.4100)\n",
      "tensor(0.5100)\n",
      "tensor(0.6100)\n",
      "tensor(0.7100)\n",
      "tensor(0.8100)\n",
      "tensor(0.9100)\n",
      "tensor(1.) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import save_image,make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "img_list = []\n",
    "for i in torch.arange(0.01, 1.01, 0.1):\n",
    "    print(i)\n",
    "    img_list.append(new_img[:, -int(512 * i), :, :, :].unsqueeze(1))\n",
    "    \n",
    "img_list = torch.cat(img_list, 1)\n",
    "this_img = img_list.reshape(100, 3, 32, 32)\n",
    "img = make_grid(img_anchor, nrow=1, padding=2, normalize=True)\n",
    "neg_img = make_grid(this_img, nrow=10, padding=2, normalize=True)\n",
    "print(img.max(), img.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bea2f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d563ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(img, 'anchor.jpg')\n",
    "save_image(neg_img, 'neg2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac04d5e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
