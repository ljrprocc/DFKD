{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a517e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import datafree\n",
    "%config InlineBackend.figure_format = 'pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a8453e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct 23 16:03:02 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 510.60.02    Driver Version: 510.60.02    CUDA Version: 11.6     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:05:00.0 Off |                  N/A |\r\n",
      "| 49%   44C    P8    20W / 350W |  21725MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:09:00.0 Off |                  N/A |\r\n",
      "| 37%   42C    P8    23W / 350W |      2MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:0A:00.0 Off |                  N/A |\r\n",
      "| 68%   65C    P2   323W / 350W |  21725MiB / 24576MiB |     99%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:82:00.0 Off |                  N/A |\r\n",
      "| 30%   36C    P8    14W / 350W |      2MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   4  NVIDIA GeForce ...  Off  | 00000000:85:00.0 Off |                  N/A |\r\n",
      "| 77%   52C    P2   118W / 350W |  22461MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   5  NVIDIA GeForce ...  Off  | 00000000:89:00.0 Off |                  N/A |\r\n",
      "| 30%   34C    P8    23W / 350W |      2MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A     55947      C   python                          21723MiB |\r\n",
      "|    2   N/A  N/A     25407      C   python                          21723MiB |\r\n",
      "|    4   N/A  N/A     41798      C   python                          22459MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7f81d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed = False\n",
    "gpu = 1\n",
    "# gpu ='0,1'\n",
    "batch_size = 128\n",
    "workers = 8\n",
    "num_classes = 10\n",
    "# num_classes = 100\n",
    "# num_classes = 200\n",
    "def prepare_model(model):\n",
    "    if not torch.cuda.is_available():\n",
    "        print('using CPU, this will be slow')\n",
    "        return model\n",
    "    elif distributed:\n",
    "        # For multiprocessing distributed, DistributedDataParallel constructor\n",
    "        # should always set the single device scope, otherwise,\n",
    "        # DistributedDataParallel will use all available devices.\n",
    "        if gpu is not None:\n",
    "#             torch.cuda.set_device(gpu)\n",
    "            model.cuda()\n",
    "            # When using a single GPU per process and per\n",
    "            # DistributedDataParallel, we need to divide the batch size\n",
    "            # ourselves based on the total number of GPUs we have\n",
    "            batch_size = int(batch_size / 1)\n",
    "            workers = int((workers + 1 - 1) / 1)\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[int(x) for x in gpu.split(',')])\n",
    "            return model\n",
    "        else:\n",
    "            model.cuda()\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "            return model\n",
    "    elif gpu is not None:\n",
    "        torch.cuda.set_device(gpu)\n",
    "        model = model.cuda(gpu)\n",
    "        return model\n",
    "    else:\n",
    "        # DataParallel will divide and allocate batch_size to all available GPUs\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddea5407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10,CIFAR100\n",
    "import datafree\n",
    "import registry\n",
    "from torch import nn\n",
    "student = registry.get_model('resnet18', num_classes=num_classes)\n",
    "teacher = registry.get_model('resnet34', num_classes=num_classes, pretrained=True).eval()\n",
    "# student = registry.get_model('wrn40_1', num_classes=num_classes)\n",
    "# student= registry.get_model('wrn16_2', num_classes=num_classes)\n",
    "# teacher = registry.get_model('wrn40_2', num_classes=num_classes)\n",
    "# normalizer = datafree.utils.Normalizer(**registry.NORMALIZE_DICT['tiny_imagenet'])\n",
    "# normalizer = datafree.utils.Normalizer(**registry.NORMALIZE_DICT['cifar10'])\n",
    "normalizer = datafree.utils.Normalizer(**registry.NORMALIZE_DICT['cifar100'])\n",
    "student = prepare_model(student)\n",
    "\n",
    "# teacher = teacher.to(gpu)\n",
    "# teacher.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "# num_ftrs = teacher.fc.in_features\n",
    "# teacher.fc = nn.Linear(num_ftrs, 200)\n",
    "# teacher.conv1 = nn.Conv2d(3,64, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "# teacher.maxpool = nn.Sequential()\n",
    "teacher = prepare_model(teacher)\n",
    "# ckpt = torch.load('../checkpoints/scratch/tiny_imagenet_resnet34_imagenet.pth', map_location='cpu')\n",
    "# dict_ckpt = dict()\n",
    "# for k, v in ckpt['state_dict'].items():\n",
    "#     dict_ckpt['.'.join(k.split('.')[1:])] = v\n",
    "# teacher.load_state_dict(dict_ckpt)\n",
    "\n",
    "teacher.load_state_dict(torch.load('../checkpoints/scratch/cifar10_resnet34.pth', map_location='cpu')['state_dict'])\n",
    "# teacher.load_state_dict(torch.load('../checkpoints/scratch/cifar10_wrn40_2.pth', map_location='cpu')['state_dict'])\n",
    "# teacher.load_state_dict(torch.load('../checkpoints/scratch/cifar100_wrn40_2.pth', map_location='cpu')['state_dict'])\n",
    "# print(ckpt['best_acc1'])\n",
    "# teacher.load_state_dict(torch.load('../checkpoints/scratch/cifar100_resnet34.pth', map_location='cpu')['state_dict'])\n",
    "teacher.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41af454a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer.pt\r\n"
     ]
    }
   ],
   "source": [
    "!ls /data/lijingru/DFKD/run/cr4_sim_normalize_pos_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "353c21e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768, 512])\n"
     ]
    }
   ],
   "source": [
    "anchor_bank = torch.load('/data/lijingru/DFKD/run/cr6_sim_normalize_pos/buffer.pt', map_location='cpu')\n",
    "# all_anchor = anchor_bank.reshape(-1, 512)\n",
    "print(anchor_bank.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3b27168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def difficulty_loss(anchor, teacher, t_out, logit_t, ds='cifar10', hard_factor=0., tau=10, device='cpu', d_neg_fea=None):\n",
    "    batch_size = anchor.size(0)\n",
    "    with torch.no_grad():\n",
    "        # t_logit, anchor_t_out = teacher(anchor.to(device).detach(), return_features=True)\n",
    "        t_logit = teacher(anchor.to(device).detach())\n",
    "        anchor_t_out = anchor.to(device)\n",
    "        # pseudo_label = pseudo_label.argmax(1)\n",
    "    # loss = 0.\n",
    "    pos_loss = 0.\n",
    "    neg_loss = 0.\n",
    "    if ds == 'cifar10':\n",
    "        normalized_anchor_t_out, normalized_t_out = F.normalize(anchor_t_out, dim=1), F.normalize(t_out, dim=1)\n",
    "        d = torch.mm(normalized_anchor_t_out, normalized_t_out.T)\n",
    "        N_an, N_batch = d.size()\n",
    "        \n",
    "        sorted_d, indice_d = torch.sort(d, dim=1)\n",
    "        d_pos = sorted_d[:, -int(0.05 * N_batch):]\n",
    "        d_neg = sorted_d[:, :-int(0.05 * N_batch)]\n",
    "        # n_neg = d_neg.size(1)\n",
    "        d_mask = torch.zeros_like(indice_d)\n",
    "        d_mask = d_mask.scatter(1, indice_d[:, -int(0.1*N_batch):], 1)\n",
    "        p_t_anchor = torch.softmax(t_logit, 1)\n",
    "        p_t_batch = torch.softmax(logit_t, 1)\n",
    "        kld_matrix = -torch.mm(p_t_anchor, p_t_batch.T.log()) + torch.diag(torch.mm(p_t_anchor, p_t_anchor.T.log())).unsqueeze(1)\n",
    "        l_kld = ((kld_matrix * d_mask).sum(1) / d_mask.sum(1)).mean()\n",
    "        # Get positive DA index\n",
    "        p_pos = torch.softmax(d_pos / tau, dim=1)\n",
    "        p_da_pos = torch.quantile(p_pos, q=1-hard_factor, dim=1).unsqueeze(1)\n",
    "        pos_loss = torch.sum(p_pos * torch.log(p_pos / p_da_pos).abs(), dim=1).mean()\n",
    "        # Get Negative DA index\n",
    "        \n",
    "        if d_neg_fea is not None:\n",
    "            d = torch.cat([d_neg, d_pos], 1)\n",
    "            d_mask = torch.zeros_like(d)\n",
    "#             d_mask[:, ]\n",
    "        p_total = torch.softmax(d / tau, dim=1)\n",
    "        # Out supervised loss.\n",
    "        print(d_mask, d_mask.shape)\n",
    "        neg_loss = -((d_mask * p_total.log()).sum(1) / (d_mask.sum(1))).mean()\n",
    "#         print(pos_loss, neg_loss, l_kld)\n",
    "        \n",
    "        return pos_loss, indice_d, neg_loss, l_kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3874fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DCGAN_Generator_CIFAR10(\n",
       "  (project): Sequential(\n",
       "    (0): Flatten()\n",
       "    (1): Linear(in_features=512, out_features=16384, bias=True)\n",
       "  )\n",
       "  (main): Sequential(\n",
       "    (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (9): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg = datafree.models.generator.DCGAN_Generator_CIFAR10(nz=512, ngf=64, nc=3, img_size=32, d=2, cond=False, type='normal', widen_factor=1)\n",
    "tg.load_state_dict(torch.load('/data/lijingru/DFKD/checkpoints/datafree-improved_cudfkd/cifar10-resnet34-resnet18--cr6_sim_normalize_pos.pth', map_location='cpu')['G'])\n",
    "prepare_model(tg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dec2cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 1, 0,  ..., 0, 1, 0],\n",
      "        [0, 0, 0,  ..., 0, 1, 0],\n",
      "        ...,\n",
      "        [0, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 1, 0]], device='cuda:1') torch.Size([768, 512])\n",
      "tensor(0.6864, device='cuda:1', grad_fn=<MeanBackward0>) tensor([[228, 303, 256,  ..., 100, 253, 286],\n",
      "        [403, 220, 114,  ..., 189, 298, 500],\n",
      "        [206, 220, 372,  ...,  48, 146, 510],\n",
      "        ...,\n",
      "        [ 15,  32, 363,  ...,  65, 456,  18],\n",
      "        [401, 312, 421,  ..., 286, 149, 315],\n",
      "        [250, 203, 403,  ...,  48, 500, 499]], device='cuda:1') tensor(4.5037, device='cuda:1', grad_fn=<NegBackward>) tensor(1.1287, device='cuda:1', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = torch.randn(512, 512).to(gpu)\n",
    "x = normalizer(tg(z))\n",
    "t_out, t_feat = teacher(x, return_features=True)\n",
    "\n",
    "a, b, c, d = difficulty_loss(anchor_bank[2], teacher.linear, t_feat, logit_t=t_out, device=gpu, hard_factor=0., tau=0.07)\n",
    "print(a, b, c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cb60d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image,make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "indexs = b[:, :-int(0.1*512)]\n",
    "img = make_grid(x[indexs[0]], nrow=7, padding=2, value_range=(0, 1))\n",
    "# plt.show(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bea2f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d563ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(img, 'neg1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac04d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
